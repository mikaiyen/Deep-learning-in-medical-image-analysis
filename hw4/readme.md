
---

# Homework 4: 頸動脈超音波影像的語意分割

本作業目標為針對經頸部超音波掃描所得的影像，進行頸動脈(Carotid Artery)的自動分割。屬於語意分割 (semantic segmentation) 問題，需針對每個像素預測其是否為頸動脈區域。

## 1. 專案概述

- **資料來源**  
  - 共有三位志願者的左、右頸部掃描影像，隨機取 100 張影像作為訓練集 (共 300 張)。  
  - 測試集由另一位志願者拍攝的 100 張影像組成。  
  - 每張影像對應一張標註 (mask) 圖片，標示頸動脈所在的像素區域。

- **資料結構**  
  - `train/pre/` 資料夾：原始超音波影像 (300 張)。  
  - `train/post/` 資料夾：對應的頸動脈標註圖 (300 張)。  
  - `train.csv`：列出訓練影像檔名與對應標註檔名。  
  - `test/pre/` 資料夾：測試影像 (100 張)。  
  - `test.csv`：測試影像的檔名。 (測試集無標註檔)

- **作業要求**  
  1. 使用 **FCN-8s (Fully Convolutional Network 8-stride)** 進行語意分割。  
  2. 使用 **U-Net** 進行語意分割。  
  3. 比較並說明這兩種結構的訓練與預測結果。

---

## 2. 資料前處理

1. **影像讀取**  
   - 利用 `train.csv` / `test.csv` 取得影像檔名，並從對應的資料夾讀取原始影像 (以及標註檔)。  
2. **資料增強 (Data Augmentation)**  
   - 可視情況對訓練影像進行如旋轉、平移、翻轉等操作，避免過擬合。  
3. **規格化 (Normalization)**  
   - 將影像像素值縮放到 [0, 1] 或 [−1, 1]。  
4. **批次資料分割 (Train / Validation)**  
   - 可將 300 張訓練影像再切分為訓練與驗證集 (如 80% / 20%)。  
   - 在實作中也可能使用交叉驗證或整合全部影像做訓練。

---

## 3. 模型與方法

### 3.1 FCN-8s

1. **基底網路 (Backbone)**  
   - 以 VGG16 結構為基底，移除後方的全連接層，保留卷積、池化層做特徵提取。  
2. **1×1 卷積**  
   - 將最深層的特徵映射 (feature map) 維度壓縮至類別數 (此處二元：前景 / 背景)。  
3. **跳階連接 (Skip Connections)**  
   - 將深層特徵 (block5) 與淺層特徵 (block4、block3) 進行融合，透過上採樣 (deconvolution/transpose convolution) 恢復空間解析度。  
   - 利用較淺層的細節訊息，增強輸出解析度。  
4. **最終上採樣**  
   - 做 8 倍上採樣，使最終輸出與原圖大小相同，進行像素層級的分類。  

### 3.2 U-Net

1. **編碼器 (Encoder)**  
   - 連續 3×3 卷積 + BatchNorm + ReLU，後接 2×2 最大池化進行下採樣。  
   - 每次下採樣後，通道數 (feature maps) 會增加。  
2. **瓶頸層 (Bottleneck)**  
   - 編碼器最深層，用於萃取高階特徵。  
3. **解碼器 (Decoder)**  
   - 透過反卷積或上採樣操作 (2 倍還原)，將特徵圖空間維度恢復。  
   - 與對應編碼器層做**跳階連接** (skip connection)，將淺層特徵融入解碼器，以保留細節。  
4. **輸出層**  
   - 最終使用 1×1 卷積將通道數轉為類別數 (此處 2)。  
   - 輸出大小與輸入影像相同。

---

## 4. 實作與訓練細節

1. **程式結構**  
   - `fcn_train.py` / `unet_train.py` (或單一 `train.py` 以參數決定使用哪個模型)。  
   - `dataset.py` (讀取並預處理資料集)。  
2. **超參數設定**  
   - **Batch Size**：4、8 (依資源調整)  
   - **Epochs**：20~50 (根據收斂情況)  
   - **Learning Rate**：1e-4 或 1e-5 (可使用 Adam / SGD)  
   - **Loss Function**：Binary Cross Entropy、Dice Loss 或結合使用  
3. **評估指標**  
   - **IoU** (Intersection over Union)  
   - **Dice coefficient** (F1-score 在像素層級)  


---

## 5. 結果與討論

1. **模型參數量**  
   - 報告指示最終模型參數約 **27,910,146**。  
   - 可使用程式 `sum(p.numel() for p in model.parameters())` 取得精確數字。
2. **再現性 (Reproducibility)**  
   - 由於資料集隨機抽樣及 GPU 運算差異，每次訓練結果略有不同，但整體趨勢相近。  
3. **訓練難度**  
   - 訓練時間較長，將資料上傳至 Kaggle 或使用其他 GPU 環境可加速處理。  
   - 調參 (learning rate、batch size) 及訓練流程 (資料增強策略) 對最終 IoU、Dice 影響甚大。
4. **FCN vs. U-Net**  
   - **FCN-8s**：跳躍連接層次較少，結構偏向 VGG16，但可兼顧高階語義與部分細節。  
   - **U-Net**：利用對稱式編碼器-解碼器結構、逐層跳接，更能保留影像細節資訊，在醫學影像分割通常表現更佳。  

---

## 6. 心得與未來改進

- **模型改良**  
  - 可嘗試 **FCN-16s / FCN-32s** (如果需要不同上採樣方式) 或 **U-Net++**、**DeepLabv3** 等更進階模型。  
- **Loss 函數**  
  - 針對高度不平衡 (背景 vs. 目標區域) 可使用 **Dice Loss** 或 **Focal Loss** 提升分割品質。  
- **資料增強**  
  - 在超音波影像中，加入光照變化、仿射變形或隨機噪聲等增強手段，可能能提升模型穩定性。

---
